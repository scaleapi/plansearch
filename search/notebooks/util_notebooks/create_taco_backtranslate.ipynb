{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EXECUTOR_URL\"] = \"***REMOVED***\"\n",
    "os.environ[\"EXECUTOR_AUTH\"] = \"***REMOVED***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from backtranslate import BackTranslateModel\n",
    "from base_classes import Problem, Test\n",
    "from exec_utils import run_tests_per_code\n",
    "from typing import Any\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = \"codegenning/finetuning-taco-plain300k-rlxf-withtest\"\n",
    "# DATASET_NAME = \"codegenning/taco-rl-tests10-easy\"\n",
    "NUM_BATCHES_TO_TRY = 3\n",
    "GEN_BATCH_SIZE = 2\n",
    "\n",
    "NUM_WORDS = 100\n",
    "PATH_TO_RESULT = f\"temp_backtranslate_gen_logs/{datetime.datetime.now().strftime('%m-%dT%H-%M-%S')}\"\n",
    "\n",
    "DATASET_NAME = \"codegenning/taco-rl-tests10-withpassingsolutions_v3\"\n",
    "data = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(data[\"train\"][\"prompt\"])\n",
    "def filter_prompt(prompt: str, delimiter: str = '\"\"\"') -> str:\n",
    "    return prompt.split(delimiter)[1].split(delimiter)[0].strip()\n",
    "\n",
    "actual_prompts = [filter_prompt(prompt) for prompt in prompts]\n",
    "actual_starter_code = list(data[\"train\"][\"starter_code\"]) \n",
    "\n",
    "tests = list(data[\"train\"][\"input_output\"])\n",
    "actual_tests = [json.loads(test_set) for test_set in tests]\n",
    "\n",
    "actual_solutions = list(data[\"train\"][\"solutions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems: list[Problem] = []\n",
    "for i, (prompt, starter_code, test, solution) in list(enumerate(zip(actual_prompts, actual_starter_code, actual_tests, actual_solutions))):\n",
    "    try:\n",
    "        problems.append(Problem.from_coderm_item(prompt, starter_code, None, tests=test, solutions=solution))\n",
    "    except:\n",
    "        new_outputs = []\n",
    "        for t in test[\"outputs\"]:\n",
    "            if isinstance(t, str):\n",
    "                new_outputs.append(t)\n",
    "            else:\n",
    "                assert isinstance(t, list)\n",
    "                new_outputs.append('\\n'.join(t) + '\\n')\n",
    "        test[\"outputs\"] = new_outputs\n",
    "        problems.append(Problem.from_coderm_item(prompt, starter_code, None, tests=test, solutions=solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btm = BackTranslateModel(model_name=\"gpt-4o-mini\", experiment_directory=PATH_TO_RESULT, cache_file=\"caches/temp_backtranslate_cache.json\", num_words=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tproblems = problems[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_problems: list[Problem] = []\n",
    "problem_to_expand_idx: list[list[int]] = []\n",
    "expand_to_problem_idx: list[int] = []\n",
    "for i, problem in enumerate(tproblems):\n",
    "    problem_to_expand_idx.append([])\n",
    "    for solution in problem.solutions:\n",
    "        problem_to_expand_idx[i].append(len(expanded_problems))\n",
    "        expanded_problems.append(Problem(problem.problem_str, problem.starter_code, problem.public_tests, problem.private_tests, [solution]))\n",
    "        expand_to_problem_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_codes = [None] * len(expanded_problems)\n",
    "selected_nl_sols = [None] * len(expanded_problems)\n",
    "unsolved_idxs = list(range(len(expanded_problems)))\n",
    "\n",
    "for iter_num in range(NUM_BATCHES_TO_TRY):\n",
    "    unsolved_problems = [problems[i] for i in unsolved_idxs]\n",
    "    tiled_problems = unsolved_problems * GEN_BATCH_SIZE \n",
    "    \n",
    "    btm.querier.set_log_directory(os.path.join(PATH_TO_RESULT, f\"iter_{iter_num}\"))\n",
    "    generated = btm.generate_solutions(tiled_problems, requery=True)\n",
    "    assert len(generated) == len(tiled_problems)\n",
    "\n",
    "    results = run_tests_per_code(generated, [problem.private_tests for problem in tiled_problems], [30] * len(tiled_problems))\n",
    "\n",
    "\n",
    "    query_path = os.path.join(PATH_TO_RESULT, f\"iter_{iter_num}\")\n",
    "    solution_files = [f for f in os.listdir(query_path) if f.startswith(\"solution\")]\n",
    "\n",
    "    solution_paths = []\n",
    "    for solution_file in solution_files:\n",
    "        solution_path = os.path.join(query_path, solution_file)\n",
    "        print(f\"Found solution file: {solution_path}\")\n",
    "        solution_paths.append(solution_path)\n",
    "    assert len(solution_paths)\n",
    "\n",
    "    nl_solutions = []\n",
    "    for path in solution_paths:\n",
    "        with open(solution_path, \"r\") as solution_file:\n",
    "            nl_sub_solutions = json.load(solution_file)\n",
    "            nl_solutions.extend([e[\"completion\"][\"text\"] for e in nl_sub_solutions])\n",
    "\n",
    "    assert len(nl_solutions) == len(results) == len(generated)\n",
    "\n",
    "\n",
    "    for i, (result, gen_code, gen_nl_sol) in enumerate(zip(results, generated, nl_solutions)):\n",
    "        original_idx = unsolved_idxs[i % len(unsolved_problems)]\n",
    "        result_good, _ = result\n",
    "        if result_good:\n",
    "            selected_codes[original_idx] = gen_code\n",
    "            selected_nl_sols[original_idx] = gen_nl_sol\n",
    "\n",
    "    unsolved_idxs = [i for i, code in enumerate(selected_codes) if code is None]\n",
    "    print(f\"Remaining 'unsolved' problems: {len(unsolved_idxs)}\")\n",
    "\n",
    "    if len(unsolved_idxs) == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test_list(tests: list[Test]) -> dict[str, Any]:\n",
    "    output_dict = {\"inputs\": [], \"outputs\": []}\n",
    "    assert len(tests)\n",
    "    for test in tests:\n",
    "        assert test.fn_name == tests[0].fn_name, \"All tests must have the same fn_name\"\n",
    "\n",
    "    fn_name = tests[0].fn_name\n",
    "    if fn_name is not None and fn_name != \"\":\n",
    "        output_dict[\"fn_name\"] = fn_name\n",
    "\n",
    "    for test in tests:\n",
    "        output_dict[\"inputs\"].append(test.get_input_no_kwargs())\n",
    "        output_dict[\"outputs\"].append(test.output)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_problems_dataset = {\"problem_str\": [], \"starter_code\": [], \"tests\": [], \"code_solutions\": [], \"nl_solutions\": []}\n",
    "\n",
    "for orig_idx, expand_idxs in enumerate(problem_to_expand_idx):\n",
    "    filtered_nl_solutions = []\n",
    "    filtered_code_solutions = []\n",
    "    for idx in expand_idxs:\n",
    "        assert (selected_nl_sols[idx] is None) == (selected_codes[idx] is None)\n",
    "        if selected_nl_sols[idx] is not None:\n",
    "            filtered_nl_solutions.append(selected_nl_sols[idx])\n",
    "            filtered_code_solutions.append(expanded_problems[idx].solutions[0])\n",
    "\n",
    "    new_problems_dataset[\"problem_str\"].append(tproblems[orig_idx].problem_str)\n",
    "    new_problems_dataset[\"starter_code\"].append(tproblems[orig_idx].starter_code)\n",
    "    new_problems_dataset[\"tests\"].append(convert_test_list(tproblems[orig_idx].private_tests))\n",
    "    new_problems_dataset[\"code_solutions\"].append(filtered_code_solutions)\n",
    "    new_problems_dataset[\"nl_solutions\"].append(filtered_nl_solutions)\n",
    "\n",
    "new_problems_dataset = Dataset.from_dict(new_problems_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_problems_dataset[\"code_solutions\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetDict({\"train\": new_problems_dataset})\n",
    "ds.push_to_hub(DATASET_NAME + \"_with_nlsols\", commit_message=\"With NL solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exec_results = run_tests_per_code(impls, [problem.private_tests for problem in expanded_problems], [30] * len(expanded_problems))\n",
    "results = [stat for stat, _ in exec_results]\n",
    "check = [c for _, c in exec_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = os.path.join(PATH_TO_RESULT, \"queries\")\n",
    "solution_files = [f for f in os.listdir(query_path) if f.startswith(\"solution\")]\n",
    "\n",
    "solution_path = None\n",
    "for solution_file in solution_files:\n",
    "    solution_path = os.path.join(query_path, solution_file)\n",
    "    print(f\"Found solution file: {solution_path}\")\n",
    "\n",
    "assert solution_path is not None\n",
    "\n",
    "with open(solution_path, \"r\") as solution_file:\n",
    "    nl_solutions = json.load(solution_file)\n",
    "\n",
    "nl_solutions = [e[\"completion\"][\"text\"] for e in nl_solutions]\n",
    "assert len(nl_solutions) == len(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlxf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
